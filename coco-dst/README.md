## Table of Contents
   - [CoCo training](#coco-training)
       - [Pretrained CoCo Model](#pretrained-coco-model)
   - [CoCo demo](#coco-demo)
   - [DST model evaluation](#dst-model-evaluation)
       - [Classifier filter training](#classifier-filter-training)
       - [TRADE](#trade)
       - [SimpleTOD](#simpletod)
       - [TripPy](#trippy)
       - [Run Evaluation on CoCo Examples](#run-evaluation-on-coco-examples)
   - [CoCo+ as data augmentation defense](#coco-as-data-augmentation-defense)
- [License](#license)
 
### CoCo training: 
To train CoCo, run
```
sh train_coco.sh
```
After training, checkpoints are save under ```OUTPUT_DIR```, you can choose the checkpoint according to perplexity on dev set. 

#### Pretrained CoCo Model:
To use the pre-trained checkpoint used in our paper: 
1. [Download](https://storage.cloud.google.com/sfr-coco-dst-research/coco-dst-resources/coco_model.zip) and uncompress it as ```coco_model```,
2. Place the resulting ```coco_model``` folder under the current module (```coco-dst/coco-dst/```).


### CoCo demo: 
To play with controllable generation on your own input, run the following command
```
sh run_demo.sh
```
You need to modify ```eval_data_file``` and ```model_name_or_path```. For ```eval_data_file```, it's the data you want 
to run the demo on. If you wish to run demo on test set, i.e. generating dialogue turns by modifying turn-level belief 
states on test set, you can keep it as is. For ```model_name_or_path```, you can use your saved checkpoint or 
refer to [using our CoCo checkpoint](#pretrained-coco-model) if you prefer to use our checkpoint.

The demo will prompt you to input data index you want to play with where you can input an index as specified in the prompt. 
Then, you can input your modified turn-level belief state or use a default one by following the specified format in 
the prompt to generate a few candidate user utterances that can extend the conversation so far reflecting the 
user-specified belief state.   

### DST model evaluation:
#### Classifier filter training
To train the classifier filter, run
```
sh train_filter.sh
```
After training, it will save best checkpoint according to highest recall on dev set, which will be be used as part of 
filtering strategy of potentially invalid utterances generated by CoCo model. 

To use our pretrained classifier filter, 
1. [Download](https://storage.cloud.google.com/sfr-coco-dst-research/coco-dst-resources/filter.zip) and uncompress it as ```filter```,
2. Place the resulting ```filter``` folder under the ```classifier_filter``` module (```coco-dst/coco-dst/classifier_filter/```).


#### TRADE
Run
```
cd ../trade-dst
```
and open ```README.md``` for more details of ```TRADE```, including its training and how to load its checkpoints used 
in our paper.

#### SimpleTOD
Run
```
cd ../simpletod
```
and open ```README.md``` for more details of ```SimpleTOD```, including its training and how to load its checkpoints 
used in our paper.

#### TripPy
Run
```
cd ../trippy-public
```
and open ```README.md``` for more details of ```TripPy```, including its training and how to load its checkpoints 
used in our paper.

#### Run Evaluation on CoCo Examples

We define arguments of our evaluation script in the beginning of  ```main()``` method of ```run_eval.py```. There are 
several key arguments, namely ```args_target_model```, ```args_method``` , ```args_slot_value_dict``` and 
```args_slot_combination_dict``` . See the comments in the script for how to set them. This script integrates data 
generation and function calls of different models for evaluation. After setting these key arguments, run
```
sh run_eval.sh
```
Generated new user utterances after filtering and evaluation results will be saved in a ```json``` file under ```args_eval_result_save_dir+"\"+args_target_model```. Note that before evaluating the model, you need to train the target models or use the checkpoints we trained in the paper. See each folder's ```README.md``` for more details about model training and how to download corresponding checkpoints.

### CoCo+ as data augmentation defense:

We also have a script ```run_gene.py``` to generate additional data for retraining as a defense mechanism. Similar to 
evaluation, we also define arguments of our data generation script in the beginning of  ```main()``` method. This 
script will provide additional training data generated by CoCo+, which can be used for retraining the 
underlying DST models.

To generate additional training data by yourself, run
```
sh run_gene.sh
```
Generated data will be saved into ```coco-dst/coco-dst/coco_data``` and can be combined with original training data 
to train DST models. See each model's ```README.md``` for more details on how to run the retraining.

OR, if you prefer to use the additional training data we already generated 
1. [Download](https://storage.cloud.google.com/sfr-coco-dst-research/coco-dst-resources/coco_data.zip) and uncompress it as ```coco_data```,
2. Place the resulting ```coco_data``` folder under the current module (```coco-dst/coco-dst/```).

## License
The code is released under BSD 3-Clause - see [LICENSE](LICENSE.txt) for details.
